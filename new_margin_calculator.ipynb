{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import tqdm\n",
    "import time\n",
    "import os\n",
    "import wandb\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from external_utils import format_time\n",
    "from utils.data_preprocessing import preprocess_dataset_get_dataset, generate_dataset_from_loader,preprocess_dataset_get_data_loader,get_data_loader\n",
    "from structure.dlgn_conv_config_structure import DatasetConfig\n",
    "import numpy as np\n",
    "import csv\n",
    "from conv4_models import get_model_instance_from_dataset, get_img_size\n",
    "from utils.forward_visualization_helpers import merge_operations_in_modules, apply_input_on_conv_matrix, merge_layers_operations_in_modules\n",
    "from sklearn import datasets, metrics, svm\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "from structure.generic_structure import SaveFeatures\n",
    "from matplotlib import colors\n",
    "from sklearn.decomposition import PCA\n",
    "from adversarial_attacks_tester import apply_adversarial_attack_on_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_attack_type = \"PGD\"\n",
    "adv_target = None\n",
    "number_of_adversarial_optimization_steps = 40\n",
    "eps = 0.3\n",
    "eps_step_size = 0.01\n",
    "update_on='all'\n",
    "rand_init=True\n",
    "norm=np.inf\n",
    "use_ytrue=True\n",
    "number_of_restarts = 1\n",
    "residue_vname = None\n",
    "is_targetted=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.max(filtered_X_train) 255.0\n",
      "filtered_X_train  1.0 0.0\n",
      "filtered_X_test  1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "model_arch_type = 'dnn'\n",
    "dataset = 'mnist'\n",
    "data_config = DatasetConfig(\n",
    "                dataset, is_normalize_data=True, valid_split_size=0.1, batch_size=256, list_of_classes=None,custom_dataset_path=None)\n",
    "\n",
    "filtered_X_train, filtered_y_train, _, _, filtered_X_test, filtered_y_test = preprocess_dataset_get_dataset(\n",
    "            data_config, model_arch_type, verbose=1, dataset_folder=\"./Datasets/\", is_split_validation=False)\n",
    "print(\"filtered_X_train \",np.max(filtered_X_train),np.min(filtered_X_train))\n",
    "print(\"filtered_X_test \",np.max(filtered_X_test),np.min(filtered_X_test))\n",
    "trainloader = get_data_loader(\n",
    "    filtered_X_train, filtered_y_train, data_config.batch_size, transforms=data_config.train_transforms)\n",
    "testloader = get_data_loader(\n",
    "    filtered_X_test, filtered_y_test, data_config.batch_size, transforms=data_config.test_transforms)\n",
    "\n",
    "# trainloader, _, testloader = preprocess_dataset_get_data_loader(\n",
    "#                 data_config, model_arch_type, verbose=1, dataset_folder=\"./Datasets/\", is_split_validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_X_train),len(filtered_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1, 28, 28])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = next(iter(trainloader))\n",
    "tst[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = tst[0][0]\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misclassified_sample_indices(model_arch_type,mpath,trainloader,fc_width=128,fc_depth = 4,pca_exp_percent=None):\n",
    "    nodes_in_each_layer_list = [fc_width] * fc_depth\n",
    "    model = get_model_instance_from_dataset(dataset,model_arch_type, seed=2022, num_classes=10, nodes_in_each_layer_list=nodes_in_each_layer_list)\n",
    "    model.load_state_dict(torch.load(mpath).state_dict())\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    acc = 0.\n",
    "    incorr_ind_list=[]\n",
    "    corr_ind_list=[]\n",
    "    \n",
    "    loader = tqdm.tqdm(trainloader, desc='Adversarial Evaluating')\n",
    "    for batch_idx, data in enumerate(loader, 0):\n",
    "        (images, labels) = data\n",
    "        images, labels = images.to(\n",
    "            device), labels.to(device)\n",
    "        \n",
    "        adv_images = apply_adversarial_attack_on_input(images, model, eps, adv_attack_type, number_of_adversarial_optimization_steps, eps_step_size, labels, is_targetted,update_on,rand_init,norm,use_ytrue,residue_vname=residue_vname)\n",
    "\n",
    "        outputs = model(adv_images)\n",
    "        \n",
    "        if(len(outputs.size())==1):\n",
    "            predicted = torch.sigmoid(outputs.data).round()\n",
    "        else:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        eq = (predicted == labels)\n",
    "        incorr_indices = (eq == False).nonzero().view(-1)+total\n",
    "        incorr_ind_list.extend(incorr_indices.cpu())\n",
    "        corr_indices = (eq == True).nonzero().view(-1)+total\n",
    "        corr_ind_list.extend(corr_indices.cpu())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    acc = 100.*correct/total\n",
    "    print(\"acc \",acc)\n",
    "\n",
    "    return  incorr_ind_list,corr_ind_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_margin_change_accuracy(model_arch_type,mpath,trainloader,fc_width=128,fc_depth = 4,pca_exp_percent=None):\n",
    "    nodes_in_each_layer_list = [fc_width] * fc_depth\n",
    "    model = get_model_instance_from_dataset(dataset,model_arch_type, seed=2022, num_classes=10, nodes_in_each_layer_list=nodes_in_each_layer_list)\n",
    "    if(pca_exp_percent is not None):\n",
    "        dataset_for_pca = generate_dataset_from_loader(trainloader)\n",
    "        if(isinstance(dataset_for_pca.list_of_x[0], torch.Tensor)):\n",
    "            dataset_for_pca = torch.stack(\n",
    "                dataset_for_pca.list_of_x), torch.stack(dataset_for_pca.list_of_y)\n",
    "        else:\n",
    "            dataset_for_pca = np.array(dataset_for_pca.list_of_x), np.array(\n",
    "                dataset_for_pca.list_of_y)\n",
    "        number_of_components_for_pca = model.initialize_PCA_transformation(\n",
    "            dataset_for_pca[0], pca_exp_percent)\n",
    "    model.load_state_dict(torch.load(mpath).state_dict())\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    correct_with_changed_beta = 0\n",
    "    total = 0\n",
    "    mod_beta = 10\n",
    "    real_beta = model.beta\n",
    "    \n",
    "    loader = tqdm.tqdm(trainloader, desc='Margin change evaluation')\n",
    "    for batch_idx, data in enumerate(loader, 0):\n",
    "        (images, labels) = data\n",
    "        images, labels = images.to(\n",
    "            device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "\n",
    "        if(len(outputs.size())==1):\n",
    "            predicted = torch.sigmoid(outputs.data).round()\n",
    "        else:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        model.beta = mod_beta\n",
    "        outputs = model(images)\n",
    "        model.beta = real_beta\n",
    "        \n",
    "        if(len(outputs.size())==1):\n",
    "            predicted = torch.sigmoid(outputs.data).round()\n",
    "        else:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        correct_with_changed_beta += (predicted == labels).sum().item()\n",
    "    \n",
    "    acc = 100.*correct/total\n",
    "    acc_changed_beta = 100.*correct_with_changed_beta/total\n",
    "    print(\"Accuracy on original examples with original model beta=4: {} and with modified beta={}: {}\".format(acc,mod_beta,acc_changed_beta))\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_margin_folder(model_path):\n",
    "    return model_path.replace(\".pt\",\"/MARGIN_ANALYSIS/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_effective_convs(model,is_include_pca=False,dummy_input=None):\n",
    "    model.eval()\n",
    "    if(dummy_input is None):\n",
    "        dummy_input = torch.rand(get_img_size(dataset)).unsqueeze(0)\n",
    "\n",
    "    dummy_input = dummy_input.to(device)\n",
    "    print(\"dummy_input \",dummy_input.size())\n",
    "    conv_matrix_operations_in_each_layer, conv_bias_operations_in_each_layer, channel_outs_size_in_each_layer = model.exact_forward_vis(dummy_input,is_include_pca)\n",
    "\n",
    "    return conv_matrix_operations_in_each_layer,conv_bias_operations_in_each_layer,channel_outs_size_in_each_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgdat_path = \"root/model/save/mnist/adversarial_training/MT_fc_dlgn_W_128_D_4_ET_ADV_TRAINING/ST_2022/fast_adv_attack_type_PGD/adv_type_PGD/EPS_0.3/batch_size_64/eps_stp_size_0.01/adv_steps_40/update_on_all/R_init_True/norm_inf/use_ytrue_True/adv_model_dir.pt\"\n",
    "# std_path = \"root/model/save/mnist/CLEAN_TRAINING/ST_2022/fc_dlgn_W_128_D_4_dir.pt\"\n",
    "std_path = \"root/model/save/mnist/adversarial_training/MT_fc_dlgn_W_128_D_4_PCA_K155_P_0.95_ET_ADV_TRAINING/ST_2022/fast_adv_attack_type_PGD/adv_type_PGD/EPS_0.3/OPT_Adam (Parameter Group 0    amsgrad: False    betas: (0.9, 0.999)    eps: 1e-08    lr: 0.0001    weight_decay: 0)/batch_size_64/eps_stp_size_0.01/adv_steps_40/update_on_all/R_init_True/norm_inf/use_ytrue_True/out_lossfn_CrossEntropyLoss()/inner_lossfn_CrossEntropyLoss()/adv_model_dir.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.gating_network Gating network  \n",
      " module_list:Linear(in_features=784, out_features=128, bias=True) \n",
      " Params in module is:100480\n",
      "Linear(in_features=128, out_features=128, bias=True) \n",
      " Params in module is:16512\n",
      "Linear(in_features=128, out_features=128, bias=True) \n",
      " Params in module is:16512\n",
      "Linear(in_features=128, out_features=128, bias=True) \n",
      " Params in module is:16512\n",
      "\n",
      "Gating net params: 150016\n",
      "self.value_network Value network  \n",
      " module_list:Linear(in_features=784, out_features=128, bias=True) \n",
      " Params in module is:100480\n",
      "Linear(in_features=128, out_features=128, bias=True) \n",
      " Params in module is:16512\n",
      "Linear(in_features=128, out_features=128, bias=True) \n",
      " Params in module is:16512\n",
      "Linear(in_features=128, out_features=128, bias=True) \n",
      " Params in module is:16512\n",
      "\n",
      "Value net params: 151306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Margin change evaluation:   0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Margin change evaluation: 100%|██████████| 235/235 [00:01<00:00, 121.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on original examples with original model beta=4: 66.08333333333333 and with modified beta=10: 66.05833333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66.08333333333333"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_margin_change_accuracy(\"fc_dlgn\",pgdat_path,trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_margin_change_accuracy(\"fc_dlgn\",std_path,trainloader,pca_exp_percent=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorr_ind_pgdat,corr_ind_pgdat = get_misclassified_sample_indices(\"fc_dlgn\",pgdat_path,trainloader)\n",
    "len(incorr_ind_pgdat),len(corr_ind_pgdat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "incorr_ind_std,corr_ind_std = get_misclassified_sample_indices(\"fc_dlgn\",std_path,trainloader)\n",
    "len(incorr_ind_std),len(corr_ind_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 1, 28, 28]), torch.Size([60000]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_X_train_tensor,filtered_y_train_tensor = torch.tensor(filtered_X_train),torch.tensor(filtered_y_train)\n",
    "filtered_X_train_tensor.size(),filtered_y_train_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1, 28, 28]), torch.Size([10]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_Xtrain_pgdat,corr_ytrain_pgdat = filtered_X_train_tensor[[corr_ind_pgdat[:10]]],filtered_y_train_tensor[[corr_ind_pgdat[:10]]]\n",
    "corr_Xtrain_pgdat.size(),corr_ytrain_pgdat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_margin(model,conv_matrix_operations_in_each_layer,outcapturer):\n",
    "    cur_batch_margins = None\n",
    "    for key,cur_m in model.get_gate_layers_ordered_dict().items():\n",
    "        cur_w = conv_matrix_operations_in_each_layer[key]\n",
    "        cur_eff_w_norm = torch.norm(cur_w,p=2,dim=1).unsqueeze(0).to(device=outcapturer[key].features.device)\n",
    "        cur_margin = torch.flatten(outcapturer[key].features,1)/cur_eff_w_norm\n",
    "        if(cur_batch_margins is None):\n",
    "            cur_batch_margins = cur_margin.T\n",
    "        else:\n",
    "            cur_batch_margins = torch.vstack([cur_batch_margins,cur_margin.T])\n",
    "\n",
    "    return cur_batch_margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flip_dist_hyperplanes_per_model(model,trainloader,number_of_components_for_pca=None):\n",
    "    outcapturer = OrderedDict()\n",
    "    for key,cur_m in model.get_gate_layers_ordered_dict().items():\n",
    "        if isinstance(cur_m, nn.Linear) or isinstance(cur_m, nn.Conv2d):\n",
    "            print(key,cur_m.weight.size(),cur_m.bias.size(),cur_m.weight.dtype,cur_m.bias.dtype)\n",
    "            outcapturer[key] = SaveFeatures(cur_m)\n",
    "\n",
    "    dummy_input = None\n",
    "    if(number_of_components_for_pca is not None):\n",
    "        dummy_input = torch.rand((1,number_of_components_for_pca)).unsqueeze(0)\n",
    "    eff_weights,eff_bias,chouts = generate_effective_convs(model,dummy_input)\n",
    "\n",
    "\n",
    "    res=None\n",
    "    loader = tqdm.tqdm(trainloader, desc='Generating Distance from HP')\n",
    "    for batch_idx, data in enumerate(loader, 0):\n",
    "        (X, y) = data\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        adv_img = apply_adversarial_attack_on_input(X, model, eps, adv_attack_type, number_of_adversarial_optimization_steps, eps_step_size, y, is_targetted,update_on,rand_init,norm,use_ytrue,residue_vname=residue_vname)\n",
    "\n",
    "        model(X)\n",
    "        dist_hp_org = torch.sign(get_batch_margin(model,eff_weights,outcapturer))\n",
    "\n",
    "        model(adv_img)\n",
    "        dist_hp_adv = torch.sign(get_batch_margin(model,eff_weights,outcapturer))\n",
    "        tmp = dist_hp_org * dist_hp_adv\n",
    "        o=torch.ones(1).to(device=tmp.device)\n",
    "        z=torch.zeros(1).to(device=tmp.device)\n",
    "        tmp=torch.where(tmp<0,o,z)\n",
    "        tmp = torch.sum(tmp,dim=-1)\n",
    "        if(res is None):\n",
    "            res = tmp\n",
    "        else:\n",
    "            res += tmp\n",
    "\n",
    "    for key,cur_m in model.get_gate_layers_ordered_dict().items():\n",
    "        if isinstance(cur_m, nn.Linear) or isinstance(cur_m, nn.Conv2d):\n",
    "            outcapturer[key].close()\n",
    "    \n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flip_dist_hyperplanes(model_arch_type,trainloader,std_path,pgd_path,fc_width=128,fc_depth = 4):\n",
    "    nodes_in_each_layer_list = [fc_width] * fc_depth\n",
    "    std_model = get_model_instance_from_dataset(dataset,model_arch_type, seed=2022, num_classes=10, nodes_in_each_layer_list=nodes_in_each_layer_list)\n",
    "    std_model.load_state_dict(torch.load(std_path).state_dict())\n",
    "    std_model = std_model.to(device)\n",
    "\n",
    "    pgdat_model = get_model_instance_from_dataset(dataset,model_arch_type, seed=2022, num_classes=10, nodes_in_each_layer_list=nodes_in_each_layer_list)\n",
    "    pgdat_model.load_state_dict(torch.load(pgd_path).state_dict())\n",
    "    pgdat_model = pgdat_model.to(device)\n",
    "\n",
    "    flip_dist_pgdat = get_flip_dist_hyperplanes_per_model(pgdat_model,trainloader)\n",
    "    print(flip_dist_pgdat.shape)\n",
    "    flip_dist_stdtr = get_flip_dist_hyperplanes_per_model(std_model,trainloader)\n",
    "    \n",
    "    save_folder = get_margin_folder(pgd_path)+\"/\"+get_margin_folder(std_path)\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    \n",
    "    fig, axs = plt.subplots(2,1,figsize=(100, 100))\n",
    "    \n",
    "    xs = np.arange(flip_dist_pgdat.shape[0])\n",
    "\n",
    "    axs[0].bar(xs, flip_dist_pgdat.detach().cpu().numpy())\n",
    "    axs[0].set_title('Flip Distribution of PGD-AT HPs',fontsize=35)\n",
    "    axs[1].set_ylabel(\"Frequency\",fontsize=35)\n",
    "    axs[0].set_xlabel(\"Hyperplane Index\",fontsize=35)\n",
    "    axs[0].set_xticks(list(np.arange(0,flip_dist_pgdat.shape[0],10)),fontsize=35)\n",
    "\n",
    "    axs[1].bar(xs, flip_dist_stdtr.detach().cpu().numpy())\n",
    "    axs[1].set_title('Flip Distribution of STD-TR HPs',fontsize=35)\n",
    "    axs[1].set_ylabel(\"Frequency\",fontsize=35)\n",
    "    axs[1].set_xlabel(\"Hyperplane Index\",fontsize=35)\n",
    "    axs[1].set_xticks(list(np.arange(0,flip_dist_stdtr.shape[0],10)),fontsize=35)\n",
    "\n",
    "    plt.savefig(save_folder+\"/hyperplane_flip_distribution.jpg\")\n",
    "    plt.savefig(save_folder+\"/hyperplane_flip_distribution.pdf\")\n",
    "\n",
    "    rows =[[a.item(),b.item()] for a,b in zip(flip_dist_pgdat,flip_dist_stdtr)]\n",
    "    with open(save_folder+\"/hyperplane_flip_distribution.csv\", 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([[\"PGD-AT flip-dist\",\"STD-TR flip-dist\"]])\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.gating_network Gating network  \n",
      " module_list:Linear(in_features=784, out_features=64, bias=True) \n",
      " Params in module is:50240\n",
      "Linear(in_features=64, out_features=64, bias=True) \n",
      " Params in module is:4160\n",
      "Linear(in_features=64, out_features=64, bias=True) \n",
      " Params in module is:4160\n",
      "Linear(in_features=64, out_features=64, bias=True) \n",
      " Params in module is:4160\n",
      "\n",
      "Gating net params: 62720\n",
      "self.value_network Value network  \n",
      " module_list:Linear(in_features=784, out_features=64, bias=True) \n",
      " Params in module is:50240\n",
      "Linear(in_features=64, out_features=64, bias=True) \n",
      " Params in module is:4160\n",
      "Linear(in_features=64, out_features=64, bias=True) \n",
      " Params in module is:4160\n",
      "Linear(in_features=64, out_features=64, bias=True) \n",
      " Params in module is:4160\n",
      "\n",
      "Value net params: 63370\n",
      "self.gating_network Gating network  \n",
      " module_list:Linear(in_features=784, out_features=64, bias=True) \n",
      " Params in module is:50240\n",
      "Linear(in_features=64, out_features=64, bias=True) \n",
      " Params in module is:4160\n",
      "Linear(in_features=64, out_features=64, bias=True) \n",
      " Params in module is:4160\n",
      "Linear(in_features=64, out_features=64, bias=True) \n",
      " Params in module is:4160\n",
      "\n",
      "Gating net params: 62720\n",
      "self.value_network Value network  \n",
      " module_list:Linear(in_features=784, out_features=64, bias=True) \n",
      " Params in module is:50240\n",
      "Linear(in_features=64, out_features=64, bias=True) \n",
      " Params in module is:4160\n",
      "Linear(in_features=64, out_features=64, bias=True) \n",
      " Params in module is:4160\n",
      "Linear(in_features=64, out_features=64, bias=True) \n",
      " Params in module is:4160\n",
      "\n",
      "Value net params: 63370\n",
      "fc0_g torch.Size([64, 784]) torch.Size([64]) torch.float32 torch.float32\n",
      "fc1_g torch.Size([64, 64]) torch.Size([64]) torch.float32 torch.float32\n",
      "fc2_g torch.Size([64, 64]) torch.Size([64]) torch.float32 torch.float32\n",
      "fc3_g torch.Size([64, 64]) torch.Size([64]) torch.float32 torch.float32\n",
      "dummy_input  torch.Size([1, 1, 28, 28])\n",
      "current_tensor_size  torch.Size([1, 784])\n",
      "current_layer Linear(in_features=784, out_features=64, bias=True)\n",
      "difference_in_output  tensor(0., device='cuda:0')\n",
      "current_layer Linear(in_features=64, out_features=64, bias=True)\n",
      "difference_in_output  tensor(5.4240e-06, device='cuda:0')\n",
      "current_layer Linear(in_features=64, out_features=64, bias=True)\n",
      "difference_in_output  tensor(6.8471e-06, device='cuda:0')\n",
      "current_layer Linear(in_features=64, out_features=64, bias=True)\n",
      "difference_in_output  tensor(7.0743e-06, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Distance from HP: 100%|██████████| 235/235 [01:24<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "fc0_g torch.Size([64, 784]) torch.Size([64]) torch.float32 torch.float32\n",
      "fc1_g torch.Size([64, 64]) torch.Size([64]) torch.float32 torch.float32\n",
      "fc2_g torch.Size([64, 64]) torch.Size([64]) torch.float32 torch.float32\n",
      "fc3_g torch.Size([64, 64]) torch.Size([64]) torch.float32 torch.float32\n",
      "dummy_input  torch.Size([1, 1, 28, 28])\n",
      "current_tensor_size  torch.Size([1, 784])\n",
      "current_layer Linear(in_features=784, out_features=64, bias=True)\n",
      "difference_in_output  tensor(0., device='cuda:0')\n",
      "current_layer Linear(in_features=64, out_features=64, bias=True)\n",
      "difference_in_output  tensor(1.4186e-05, device='cuda:0')\n",
      "current_layer Linear(in_features=64, out_features=64, bias=True)\n",
      "difference_in_output  tensor(1.7697e-05, device='cuda:0')\n",
      "current_layer Linear(in_features=64, out_features=64, bias=True)\n",
      "difference_in_output  tensor(2.7776e-05, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Distance from HP:  50%|████▉     | 117/235 [00:45<00:45,  2.58it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# pgdat_path = \"root/model/save/mnist/adversarial_training/MT_fc_dlgn_W_128_D_4_ET_ADV_TRAINING/ST_2022/fast_adv_attack_type_PGD/adv_type_PGD/EPS_0.3/batch_size_64/eps_stp_size_0.01/adv_steps_40/update_on_all/R_init_True/norm_inf/use_ytrue_True/adv_model_dir.pt\"\n",
    "# pgdat_path = \"root/model/save/fashion_mnist/adversarial_training/MT_fc_dlgn_W_128_D_4_ET_ADV_TRAINING/ST_2022/fast_adv_attack_type_PGD/adv_type_PGD/EPS_0.3/OPT_Adam (Parameter Group 0    amsgrad: False    betas: (0.9, 0.999)    eps: 1e-08    lr: 0.0001    weight_decay: 0)/batch_size_64/eps_stp_size_0.01/adv_steps_40/update_on_all/R_init_True/norm_inf/use_ytrue_True/out_lossfn_CrossEntropyLoss()/inner_lossfn_CrossEntropyLoss()/adv_model_dir.pt\"\n",
    "pgdat_path = \"root/model/save/mnist/adversarial_training/MT_fc_dlgn_W_64_D_4_ET_ADV_TRAINING/ST_2022/fast_adv_attack_type_PGD/adv_type_PGD/EPS_0.3/OPT_Adam (Parameter Group 0    amsgrad: False    betas: (0.9, 0.999)    eps: 1e-08    lr: 0.0001    weight_decay: 0)/batch_size_64/eps_stp_size_0.01/adv_steps_40/update_on_all/R_init_True/norm_inf/use_ytrue_True/out_lossfn_CrossEntropyLoss()/inner_lossfn_CrossEntropyLoss()/adv_model_dir.pt\"\n",
    "# std_path = \"root/model/save/mnist/CLEAN_TRAINING/ST_2022/fc_dlgn_W_128_D_4_dir.pt\"\n",
    "std_path = \"root/model/save/mnist/CLEAN_TRAINING/ST_2022/fc_dlgn_W_64_D_4_dir.pt\"\n",
    "get_flip_dist_hyperplanes(\"fc_dlgn\",trainloader,std_path,pgdat_path,fc_width=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_sample_dist_info(model,sample_ind):\n",
    "    X,y = filtered_X_train_tensor[sample_ind].unsqueeze(0).to(device),filtered_y_train_tensor[sample_ind].unsqueeze(0).to(device)\n",
    "    print(X.size(),y.size())\n",
    "    org_pred = torch.max(model(X).data, 1)[1].item()\n",
    "    adv_img = apply_adversarial_attack_on_input(X, model, eps, adv_attack_type, number_of_adversarial_optimization_steps, eps_step_size, y, is_targetted,update_on,rand_init,norm,use_ytrue,residue_vname=residue_vname)\n",
    "    adv_pred = torch.max(model(adv_img).data, 1)[1].item()\n",
    "\n",
    "    eff_weights,eff_bias,chouts = generate_effective_convs(model)\n",
    "\n",
    "    outcapturer = OrderedDict()\n",
    "    for key,cur_m in model.get_gate_layers_ordered_dict().items():\n",
    "        if isinstance(cur_m, nn.Linear) or isinstance(cur_m, nn.Conv2d):\n",
    "            print(key,cur_m.weight.size(),cur_m.bias.size())\n",
    "            outcapturer[key] = SaveFeatures(cur_m)\n",
    "\n",
    "    model(X)\n",
    "    dist_hp_org = get_batch_margin(model,eff_weights,outcapturer)\n",
    "\n",
    "    model(adv_img)\n",
    "    dist_hp_adv = get_batch_margin(model,eff_weights,outcapturer)\n",
    "\n",
    "    for key,cur_m in model.get_gate_layers_ordered_dict().items():\n",
    "        if isinstance(cur_m, nn.Linear) or isinstance(cur_m, nn.Conv2d):\n",
    "            outcapturer[key].close()\n",
    "\n",
    "    return org_pred,adv_pred,adv_img,adv_img - X,dist_hp_org.squeeze(-1),dist_hp_adv.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_wise_hp_dist_std_vs_pgd(model_arch_type,sample_ind,std_path,pgd_path,fc_width=128,fc_depth = 4):\n",
    "    nodes_in_each_layer_list = [fc_width] * fc_depth\n",
    "    std_model = get_model_instance_from_dataset(dataset,model_arch_type, seed=2022, num_classes=10, nodes_in_each_layer_list=nodes_in_each_layer_list)\n",
    "    std_model.load_state_dict(torch.load(std_path).state_dict())\n",
    "    std_model = std_model.to(device)\n",
    "\n",
    "    pgdat_model = get_model_instance_from_dataset(dataset,model_arch_type, seed=2022, num_classes=10, nodes_in_each_layer_list=nodes_in_each_layer_list)\n",
    "    pgdat_model.load_state_dict(torch.load(pgd_path).state_dict())\n",
    "    pgdat_model = pgdat_model.to(device)\n",
    "\n",
    "    org_label = filtered_y_train_tensor[sample_ind].to(device)\n",
    "    \n",
    "    std_org_pred,std_adv_pred,std_adv_img,pert_std,dist_hp_std_org,dist_hp_std_adv = get_per_sample_dist_info(std_model,sample_ind)\n",
    "    pgdat_org_pred,pgdat_adv_pred,pgdat_adv_img,pert_pgdat,dist_hp_pgdat_org,dist_hp_pgdat_adv = get_per_sample_dist_info(pgdat_model,sample_ind)\n",
    "    \n",
    "    save_folder = get_margin_folder(pgd_path)+\"/\"+get_margin_folder(std_path)+\"/Sind_{}_c_{}_stdadv_pr_{}_pgdadv_pr_{}\".format(sample_ind,org_label,std_adv_pred,pgdat_adv_pred)\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    \n",
    "    width = 0.5\n",
    "    hw=0.2\n",
    "    fig, axs = plt.subplots(2,2,figsize=(50, 50))\n",
    "\n",
    "    im=axs[0,0].imshow(pert_std[0][0].detach().cpu().numpy(), interpolation='nearest')\n",
    "    axs[0,0].set_title(\"Perturbation image:{} on STD-TR\".format(sample_ind),fontsize=35)\n",
    "    plt.colorbar(im, ax=axs[0,0])\n",
    "\n",
    "    im=axs[0,1].imshow(std_adv_img[0][0].detach().cpu().numpy(), interpolation='nearest')\n",
    "    axs[0,1].set_title(\"Adversarial image:{} on STD-TR\".format(sample_ind),fontsize=35)\n",
    "    plt.colorbar(im, ax=axs[0,1])\n",
    "\n",
    "    im=axs[1,0].imshow(pert_pgdat[0][0].detach().cpu().numpy(), interpolation='nearest')\n",
    "    axs[1,0].set_title(\"Perturbation image:{} on PGD-AT\".format(sample_ind),fontsize=35)\n",
    "    plt.colorbar(im, ax=axs[1,0])\n",
    "\n",
    "    im=axs[1,1].imshow(pgdat_adv_img[0][0].detach().cpu().numpy(), interpolation='nearest')\n",
    "    axs[1,1].set_title(\"Adversarial image:{} on PGD-AT\".format(sample_ind),fontsize=35)\n",
    "    plt.colorbar(im, ax=axs[1,1])\n",
    "\n",
    "    plt.savefig(save_folder+\"/images_std_pgdat_ind_{}_c_{}_stdadv_pr_{}_pgdadv_pr_{}.jpg\".format(sample_ind,org_label,std_adv_pred,pgdat_adv_pred))\n",
    "    plt.savefig(save_folder+\"/images_std_pgdat_ind_{}_c_{}_stdadv_pr_{}_pgdadv_pr_{}.pdf\".format(sample_ind,org_label,std_adv_pred,pgdat_adv_pred))\n",
    "\n",
    "    rows =[[a.item(),b.item(),c.item(),d.item()] for a,b,c,d in zip(dist_hp_std_org,dist_hp_std_adv,dist_hp_pgdat_org,dist_hp_pgdat_adv)]\n",
    "    with open(save_folder+\"/distance_from_HP_stats_ind_{}_c_{}_stdadv_pr_{}_pgdadv_pr_{}.csv\".format(sample_ind,org_label,std_adv_pred,pgdat_adv_pred), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows([[\"STD-TR org\",\"STD-TR adv\",\"PGD-AT org\",\"PGD-AT adv\"]])\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    fig, axs = plt.subplots(4,1,figsize=(100, 100))\n",
    "    \n",
    "    xs = np.arange(dist_hp_std_org.shape[0])\n",
    "    \n",
    "    axs[0].bar(xs, dist_hp_std_org.detach().cpu().numpy())\n",
    "    axs[0].set_title('Distance from HP STD-TR original image:{}'.format(sample_ind),fontsize=35)\n",
    "    axs[0].set_xlabel(\"Hyperplane Index\",fontsize=35)\n",
    "    axs[0].set_xticks(list(np.arange(0,dist_hp_std_org.shape[0],10)),fontsize=35)\n",
    "\n",
    "    axs[1].bar(xs, dist_hp_std_adv.detach().cpu().numpy())\n",
    "    axs[1].set_title('Distance from HP STD-TR adversarial image:{}'.format(sample_ind),fontsize=35)\n",
    "    axs[1].set_xlabel(\"Hyperplane Index\",fontsize=35)\n",
    "    axs[1].set_xticks(list(np.arange(0,dist_hp_std_org.shape[0],10)),fontsize=35)\n",
    "\n",
    "    xs = np.arange(dist_hp_std_org.shape[0])\n",
    "\n",
    "    axs[2].bar(xs, dist_hp_pgdat_org.detach().cpu().numpy())\n",
    "    axs[2].set_title('Distance from HP PGD-AT original image:{}'.format(sample_ind),fontsize=35)\n",
    "    axs[2].set_xlabel(\"Hyperplane Index\",fontsize=35)\n",
    "    axs[2].set_xticks(list(np.arange(0,dist_hp_std_org.shape[0],10)),fontsize=35)\n",
    "\n",
    "    axs[3].bar(xs, dist_hp_pgdat_adv.detach().cpu().numpy())\n",
    "    axs[3].set_title('Distance from HP PGD-AT adversarial image:{}'.format(sample_ind),fontsize=35)\n",
    "    axs[3].set_xlabel(\"Hyperplane Index\",fontsize=35)\n",
    "    axs[3].set_xticks(list(np.arange(0,dist_hp_std_org.shape[0],10)),fontsize=35)\n",
    "\n",
    "    print(\"org_label:{}    std_org_pred:{}    std_adv_pred:{}\".format(org_label,std_org_pred,std_adv_pred))\n",
    "    print(\"org_label:{}    pgdat_org_pred:{}  pgdat_adv_pred:{}\".format(org_label,pgdat_org_pred,pgdat_adv_pred))\n",
    "\n",
    "    \n",
    "    plt.savefig(save_folder+\"/distance_from_HP_stats_ind_{}_c_{}_stdadv_pr_{}_pgdadv_pr_{}.jpg\".format(sample_ind,org_label,std_adv_pred,pgdat_adv_pred))\n",
    "    plt.savefig(save_folder+\"/distance_from_HP_stats_ind_{}_c_{}_stdadv_pr_{}_pgdadv_pr_{}.pdf\".format(sample_ind,org_label,std_adv_pred,pgdat_adv_pred))\n",
    "    print(\"Stored at \",save_folder)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_class_indices=[None]*10\n",
    "i=0\n",
    "cnt=0\n",
    "while(cnt<10):\n",
    "    cl = filtered_y_train_tensor[corr_ind_pgdat[i].item()]\n",
    "    if(all_class_indices[cl] is None):\n",
    "        all_class_indices[cl]=i\n",
    "        cnt+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in all_class_indices:\n",
    "    plot_sample_wise_hp_dist_std_vs_pgd(\"fc_dlgn\",corr_ind_pgdat[ind].item(),std_path,pgdat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-work-DAG-DNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
