{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2024-10-02 02:05:57.474723: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2024-10-02 02:05:57.474906: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2024-10-02 02:05:57.474919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-10-02 02:05:58.538393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2024-10-02 02:05:58.618656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:89:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-10-02 02:05:58.618856: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2024-10-02 02:05:58.618967: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\n",
      "2024-10-02 02:05:58.619019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2024-10-02 02:05:58.619048: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2024-10-02 02:05:58.619176: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
      "2024-10-02 02:05:58.619271: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\n",
      "2024-10-02 02:05:58.619366: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-02 02:05:58.619377: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-10-02 02:05:58.619958: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2024-10-02 02:05:58.635054: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2194875000 Hz\n",
      "2024-10-02 02:05:58.640454: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561867cf9f00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-02 02:05:58.640500: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-10-02 02:05:58.839730: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561864e897d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-02 02:05:58.839776: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2024-10-02 02:05:58.839940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-10-02 02:05:58.839950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import tqdm\n",
    "import time\n",
    "import os\n",
    "import wandb\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from external_utils import format_time\n",
    "from utils.data_preprocessing import preprocess_dataset_get_dataset, generate_dataset_from_loader,preprocess_dataset_get_data_loader,get_data_loader\n",
    "from structure.dlgn_conv_config_structure import DatasetConfig\n",
    "import numpy as np\n",
    "import csv\n",
    "from conv4_models import get_model_instance_from_dataset, get_img_size\n",
    "from utils.forward_visualization_helpers import merge_operations_in_modules, apply_input_on_conv_matrix, merge_layers_operations_in_modules\n",
    "from sklearn import datasets, metrics, svm\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "from structure.generic_structure import SaveFeatures\n",
    "from matplotlib import colors\n",
    "from sklearn.decomposition import PCA\n",
    "from adversarial_attacks_tester import apply_adversarial_attack_on_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (0.42.1)\n",
      "Requirement already satisfied: packaging>20.9 in /home/akshay/.local/lib/python3.7/site-packages (from shap) (24.0)\n",
      "Requirement already satisfied: pandas in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from shap) (1.3.5)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from shap) (4.64.0)\n",
      "Requirement already satisfied: scipy in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from shap) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from shap) (1.0.2)\n",
      "Requirement already satisfied: numba in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from shap) (0.56.4)\n",
      "Requirement already satisfied: numpy in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from shap) (1.21.5)\n",
      "Requirement already satisfied: slicer==0.0.7 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: cloudpickle in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from shap) (2.2.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from numba->shap) (0.39.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from numba->shap) (4.11.3)\n",
      "Requirement already satisfied: setuptools in /home/akshay/.local/lib/python3.7/site-packages (from numba->shap) (68.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from importlib-metadata->numba->shap) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from importlib-metadata->numba->shap) (4.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/akshay/.local/lib/python3.7/site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from pandas->shap) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/akshay/.local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->shap) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from scikit-learn->shap) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/akshay/.conda/envs/research-work-DAG-DNN/lib/python3.7/site-packages (from scikit-learn->shap) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.max(filtered_X_train) 255.0\n",
      "filtered_X_train  1.0 0.0\n",
      "filtered_X_test  1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "model_arch_type = 'dnn'\n",
    "dataset = 'fashion_mnist'\n",
    "data_config = DatasetConfig(\n",
    "                dataset, is_normalize_data=True, valid_split_size=0.1, batch_size=8, list_of_classes=None,custom_dataset_path=None)\n",
    "\n",
    "filtered_X_train, filtered_y_train, _, _, filtered_X_test, filtered_y_test = preprocess_dataset_get_dataset(\n",
    "            data_config, model_arch_type, verbose=1, dataset_folder=\"./Datasets/\", is_split_validation=False)\n",
    "print(\"filtered_X_train \",np.max(filtered_X_train),np.min(filtered_X_train))\n",
    "print(\"filtered_X_test \",np.max(filtered_X_test),np.min(filtered_X_test))\n",
    "trainloader = get_data_loader(\n",
    "    filtered_X_train, filtered_y_train, data_config.batch_size, transforms=data_config.train_transforms)\n",
    "testloader = get_data_loader(\n",
    "    filtered_X_test, filtered_y_test, data_config.batch_size, transforms=data_config.test_transforms)\n",
    "\n",
    "# trainloader, _, testloader = preprocess_dataset_get_data_loader(\n",
    "#                 data_config, model_arch_type, verbose=1, dataset_folder=\"./Datasets/\", is_split_validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Wrapper_Module(nn.Module):\n",
    "    def __init__(self, actual_model):\n",
    "        super(Loss_Wrapper_Module, self).__init__()\n",
    "        self.actual_model = actual_model\n",
    "    \n",
    "    def forward(self, gating_signals=None,inp=None):\n",
    "        if(type(gating_signals)!=list):\n",
    "            gating_signals = torch.split(gating_signals,gating_signals.size()[-1]//len(self.actual_model.list_of_modules),-1)\n",
    "        if(inp is None):\n",
    "            inp = torch.ones(gating_signals[0].shape[0],1,self.actual_model.input_size).to(device=gating_signals[0].device)\n",
    "        \n",
    "        outputs = self.actual_model(inp,gating_signals)\n",
    "        # print(outputs.size())\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shapely_values(model_arch_type,mpath,trainloader,fc_width=128,fc_depth = 4,pca_exp_percent=None):\n",
    "    nodes_in_each_layer_list = [fc_width] * fc_depth\n",
    "    model = get_model_instance_from_dataset(dataset,model_arch_type, seed=2022, num_classes=10, nodes_in_each_layer_list=nodes_in_each_layer_list)\n",
    "    model.load_state_dict(torch.load(mpath).state_dict())\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Define the loss function (CrossEntropyLoss)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    sample_data, sample_labels = tst = next(iter(trainloader))\n",
    "    sample_data = sample_data.to(device)\n",
    "    sample_labels = sample_labels.to(device)\n",
    "    print(\"sample_data \",sample_data.size())\n",
    "\n",
    "    inp_gating = torch.ones(sample_data.size(),requires_grad=True, device=device)\n",
    "\n",
    "    linear_conv_outputs, _ = model.gating_network(sample_data)\n",
    "\n",
    "    feature_map = [None] * len(linear_conv_outputs)\n",
    "    for indx in range(len(linear_conv_outputs)):\n",
    "        each_linear_conv_output = linear_conv_outputs[indx]\n",
    "        feature_map[indx] = nn.Sigmoid()(\n",
    "            model.beta * each_linear_conv_output)\n",
    "    feature_map = torch.cat(feature_map,dim=-1)\n",
    "    print(\"feature_map \",feature_map.size())\n",
    "\n",
    "    valnet = Loss_Wrapper_Module(model.value_network)\n",
    "    output = valnet(feature_map,inp_gating)\n",
    "\n",
    "    explainer = shap.DeepExplainer(valnet, feature_map)\n",
    "    shap_values = explainer.shap_values(feature_map)\n",
    "    \n",
    "    return shap_values,feature_map.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.gating_network Gating network  \n",
      " module_list:Linear(in_features=784, out_features=128, bias=True) \n",
      " Params in module is:100480\n",
      "Linear(in_features=128, out_features=128, bias=True) \n",
      " Params in module is:16512\n",
      "Linear(in_features=128, out_features=128, bias=True) \n",
      " Params in module is:16512\n",
      "Linear(in_features=128, out_features=128, bias=True) \n",
      " Params in module is:16512\n",
      "\n",
      "Gating net params: 150016\n",
      "self.value_network Value network  \n",
      " module_list:Linear(in_features=784, out_features=128, bias=True) \n",
      " Params in module is:100480\n",
      "Linear(in_features=128, out_features=128, bias=True) \n",
      " Params in module is:16512\n",
      "Linear(in_features=128, out_features=128, bias=True) \n",
      " Params in module is:16512\n",
      "Linear(in_features=128, out_features=128, bias=True) \n",
      " Params in module is:16512\n",
      "\n",
      "Value net params: 151306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data  torch.Size([8, 1, 28, 28])\n",
      "feature_map  torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "pgdat_path = \"root/model/save/fashion_mnist/adversarial_training/MT_fc_dlgn_W_128_D_4_ET_ADV_TRAINING/ST_2022/fast_adv_attack_type_PGD/adv_type_PGD/EPS_0.3/OPT_Adam (Parameter Group 0    amsgrad: False    betas: (0.9, 0.999)    eps: 1e-08    lr: 0.0001    weight_decay: 0)/batch_size_64/eps_stp_size_0.01/adv_steps_40/update_on_all/R_init_True/norm_inf/use_ytrue_True/out_lossfn_CrossEntropyLoss()/inner_lossfn_CrossEntropyLoss()/adv_model_dir.pt\"\n",
    "shap_values,fm = get_shapely_values(\"fc_dlgn\",pgdat_path,trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " [(8, 512),\n",
       "  (8, 512),\n",
       "  (8, 512),\n",
       "  (8, 512),\n",
       "  (8, 512),\n",
       "  (8, 512),\n",
       "  (8, 512),\n",
       "  (8, 512),\n",
       "  (8, 512),\n",
       "  (8, 512)],\n",
       " (8, 512))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shap_values),[i.shape for i in shap_values],fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 4), (128, 4))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values[0][0].reshape(128,4).shape, fm[0].reshape(128,4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.image_plot(shap_values[0][0].reshape(1,128,4), fm[0].reshape(1,128,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-work-DAG-DNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
